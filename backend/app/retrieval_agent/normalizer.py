import pandas as pd
import logging
from typing import Any

logger = logging.getLogger(__name__)

TARGET_COLUMNS = [
    'Application_ID',
    'Date_Announced',
    'CIA_Name',
    'Grant_Title',
    'Grant_Type',
    'Total_Amount',
    'Broad_Research_Area',
    'Field_of_Research',
    'Plain_Description',
    'Admin_Institution',
    'Grant_Start_Year',
    'Grant_End_Date',
    'CIA_ORCID_ID',
    'Funding_Body',
    'Source_File',
    'Grant_Status',
    'Investigators',
    'Participating_Institutions'
]

# Hardcoded mappings for known formats (much faster than AI calls)
KNOWN_MAPPINGS = {
    'multi_sheet_2020_plus': {
        'Application ID': 'Application_ID',
        'Date Announced': 'Date_Announced',
        'Chief Investigator A (Project Lead)': 'CIA_Name',
        'Grant Title': 'Grant_Title',
        'Funding Scheme': 'Grant_Type',
        'Total amount awarded': 'Total_Amount',
        'Broad Research Area': 'Broad_Research_Area',
        'All_Fields': 'Field_of_Research',
        'Fields of Research': 'Field_of_Research', # Fallback
        'Plain Description': 'Plain_Description',
        'Administering Institution': 'Admin_Institution',
        'Grant Start Date': 'Grant_Start_Year',
        'Grant End Date': 'Grant_End_Date',
        'CIA ORCID ID': 'CIA_ORCID_ID',
        'All_Investigators': 'Investigators',
        'All_Institutions': 'Participating_Institutions',
    },
    'legacy_standard': {
        'APP ID': 'Application_ID',
        'App ID': 'Application_ID',
        'Date Announced': 'Date_Announced',
        'CIA Name': 'CIA_Name',
        'CIA_Name': 'CIA_Name',
        'Grant Title': 'Grant_Title',
        'Grant Type': 'Grant_Type',
        'Total': 'Total_Amount',
        'Broad Research Area': 'Broad_Research_Area',
        'Field of Research': 'Field_of_Research',
        'Fields of Research': 'Field_of_Research',
        'Field(s) of Research': 'Field_of_Research',
        'Plain Description': 'Plain_Description',
        'Admin Institution': 'Admin_Institution',
        'Start Yr': 'Grant_Start_Year',
        'End Yr': 'Grant_End_Date',
    },
    'legacy_2014': {
        'APP ID': 'Application_ID',
        'Date Announced': 'Date_Announced',
        'CIA_NAME': 'CIA_Name',
        'SIMPLIFIED TITLE': 'Grant_Title',
        'GRANT_TYPE': 'Grant_Type',
        'TOTAL': 'Total_Amount',
        'BROAD_RESEARCH_AREA': 'Broad_Research_Area',
        'FIELD_OF_  RESEARCH': 'Field_of_Research',
        'ADMIN_ INSTITUTION': 'Admin_Institution',
    },
    'arc_api': {
        'code': 'Application_ID',
        'funding-commencement-year': 'Grant_Start_Year',
        'lead-investigator': 'CIA_Name',
        'scheme-name': 'Grant_Type',
        'announced-funding-amount': 'Total_Amount',
        'primary-field-of-research': 'Field_of_Research',
        'national-interest-test-statement': 'Plain_Description',
        'current-admin-organisation': 'Admin_Institution',
        'anticipated-end-date': 'Grant_End_Date',
        'investigators': 'Investigators',
        'grant-status': 'Grant_Status',
        'Funding_Body': 'Funding_Body',
        'grant-summary': 'Grant_Title',
    }
}

# Cache for mappings generated by AI
_mapping_cache = {}


def detect_format(headers: list) -> str:
    """Detect which known format this file uses."""
    headers_set = set(headers)
    
    # Check for multi-sheet 2020+ format (2025 file)
    if 'Application ID' in headers_set and 'Chief Investigator A (Project Lead)' in headers_set:
        return 'multi_sheet_2020_plus'
        
    # Check for legacy 2014 format
    if 'CIA_NAME' in headers_set and 'GRANT_TYPE' in headers_set:
        return 'legacy_2014'
        
    # Check for standard legacy format (covers most years)
    # Check for variations of APP ID and CIA Name
    has_app_id = 'APP ID' in headers_set or 'App ID' in headers_set
    has_cia = 'CIA Name' in headers_set or 'CIA_Name' in headers_set
    
    if has_app_id and has_cia:
        return 'legacy_standard'
    
    if 'code' in headers_set and 'scheme-name' in headers_set:
        return 'arc_api'
    
    return 'unknown'


def get_mapping_fast(client: Any, model_name: str, current_headers: list[str]) -> dict[str, str]:
    """
    Get column mapping quickly by:
    1. Checking if it's a known format (instant)
    2. Checking the cache (instant)
    3. Falling back to AI (slow)
    """
    # Convert headers to string for safe sorting and caching
    # Fixes TypeError: '<' not supported between instances of 'int' and 'str'
    safe_headers = [str(h) for h in current_headers]
    headers_key = tuple(sorted(safe_headers))
    
    # Check cache first
    if headers_key in _mapping_cache:
        logger.info("Using cached mapping")
        return _mapping_cache[headers_key]
    
    # Detect known formats (pass original headers to keep types if needed, or safe headers)
    # safe_headers works fine for detection too as we check for string existence
    format_type = detect_format(safe_headers)
    if format_type != 'unknown':
        logger.info(f"Using hardcoded mapping for format: {format_type}")
        mapping = KNOWN_MAPPINGS[format_type].copy()
        _mapping_cache[headers_key] = mapping
        return mapping
    
    # Fall back to AI mapping
    logger.info("Unknown format, using AI mapping")
    mapping = map_columns_with_agent(client, model_name, current_headers)
    _mapping_cache[headers_key] = mapping
    return mapping


def smart_load_excel_with_joins(filepath: str) -> pd.DataFrame:
    """
    Intelligently loads and joins data from multi-sheet Excel files.
    For newer files (2020+), joins GRANTS DATA, Fields of Research, Chief Investigators, and Institutions.
    For older files, falls back to single-sheet loading.
    """
    try:
        xls = pd.ExcelFile(filepath)
        sheet_names = xls.sheet_names
        
        # Check if this is a multi-sheet file (newer format)
        required_sheets = ['GRANTS DATA', 'Chief Investigators', 'Institutions']
        has_multi_sheets = all(sheet in sheet_names for sheet in required_sheets)
        
        if has_multi_sheets:
            logger.info(f"File {filepath}: Loading multi-sheet format with joins")
            
            # Load main grants data
            df_grants = pd.read_excel(xls, sheet_name='GRANTS DATA')
            logger.info(f"  - Loaded GRANTS DATA: {len(df_grants)} rows")
            
            # Load and aggregate Chief Investigators
            if 'Chief Investigators' in sheet_names:
                df_investigators = pd.read_excel(xls, sheet_name='Chief Investigators')
                # Group investigators by Application ID into a list
                investigators_grouped = df_investigators.groupby('Application ID').agg({
                    'Chief Investigator Name': lambda x: ' | '.join(x.astype(str))
                }).reset_index()
                investigators_grouped.columns = ['Application ID', 'All_Investigators']
                df_grants = df_grants.merge(investigators_grouped, on='Application ID', how='left')
                logger.info(f"  - Joined Chief Investigators: {len(df_investigators)} rows")
            
            # Load and aggregate Institutions
            if 'Institutions' in sheet_names:
                df_institutions = pd.read_excel(xls, sheet_name='Institutions')
                # Group institutions by Application ID into a list
                institutions_grouped = df_institutions.groupby('Application ID').agg({
                    'Institution Name': lambda x: ' | '.join(x.astype(str))
                }).reset_index()
                institutions_grouped.columns = ['Application ID', 'All_Institutions']
                df_grants = df_grants.merge(institutions_grouped, on='Application ID', how='left')
                logger.info(f"  - Joined Institutions: {len(df_institutions)} rows")
            
            # Load and aggregate Fields of Research (if exists)
            if 'Fields of Research' in sheet_names:
                df_fields = pd.read_excel(xls, sheet_name='Fields of Research')
                # Group fields by Application ID into a list
                fields_grouped = df_fields.groupby('Application ID').agg({
                    'Field': lambda x: ' | '.join(x.astype(str))
                }).reset_index()
                fields_grouped.columns = ['Application ID', 'All_Fields']
                df_grants = df_grants.merge(fields_grouped, on='Application ID', how='left')
                logger.info(f"  - Joined Fields of Research: {len(df_fields)} rows")
            
            return df_grants
        else:
            # Older format - use previous logic
            return smart_load_excel_single_sheet(filepath, xls, sheet_names)
            
    except Exception as e:
        logger.error(f"Error loading {filepath}: {e}")
        # Last resort fallback
        return pd.read_excel(filepath)


def smart_load_excel_single_sheet(filepath: str, xls: pd.ExcelFile, sheet_names: list) -> pd.DataFrame:
    """
    Loads a single sheet from older format Excel files.
    Tries to find the correct header row.
    """
    def find_header_row(df_preview):
        # iterate rows to find header
        for i, row in df_preview.iterrows():
            row_str = row.astype(str).str.lower().str.cat(sep=' ')
            if 'app id' in row_str or 'application id' in row_str or 'cia name' in row_str:
                return i + 1 # +1 because preview index is relative to sheet start, assuming 0-based
        return 0

    # 1. Try explicit names
    priority_sheets = ['GRANTS DATA', 'Grants Data', 'Data']
    for name in priority_sheets:
        if name in sheet_names:
            logger.info(f"File {filepath}: Found priority sheet '{name}'")
            # Check for header row in first 10 rows
            df_preview = pd.read_excel(xls, sheet_name=name, nrows=10, header=None)
            for i, row in df_preview.iterrows():
                row_str = row.astype(str).str.lower().str.cat(sep=' ')
                if 'app id' in row_str or 'application id' in row_str or 'cia name' in row_str:
                     logger.info(f"  - Found header at row {i}")
                     return pd.read_excel(xls, sheet_name=name, header=i)
            return pd.read_excel(xls, sheet_name=name)
    
    # 2. Heuristic search: Look for sheets with 'App ID' or 'Application ID' in first few rows
    for name in sheet_names:
        try:
            # Read header area
            df_preview = pd.read_excel(xls, sheet_name=name, nrows=10, header=None)
            
            for i, row in df_preview.iterrows():
                row_str = row.astype(str).str.lower().str.cat(sep=' ')
                if 'app id' in row_str or 'application id' in row_str or 'cia name' in row_str:
                    logger.info(f"File {filepath}: Found data keywords in sheet '{name}' at row {i}")
                    return pd.read_excel(xls, sheet_name=name, header=i)
        except Exception:
            continue
            
    # 3. Fallback: First sheet
    logger.info(f"File {filepath}: No 'Data' sheet found, defaulting to first sheet '{sheet_names[0]}'")
    return pd.read_excel(xls, sheet_name=0)


# Keep the old function for backwards compatibility
def smart_load_excel(filepath: str) -> pd.DataFrame:
    """
    Wrapper function for backwards compatibility.
    """
    return smart_load_excel_with_joins(filepath)

def map_columns_with_agent(client: Any, model_name: str, current_headers: list[str]) -> dict[str, str]:
    """
    Uses the provided google.genai client to map current headers to target headers.
    Returns a dict: {current_header_name: target_header_name}
    Only includes mappings where a match is found.
    """
    
    prompt = f"""
    You are an expert data analyst. 
    I have a list of column headers from a grant outcome Excel file. 
    I need to map them to a standardized schema.
    
    Target Schema Definitions:
    - Application_ID: Unique identifier for the application (e.g., App ID, Application ID).
    - Date_Announced: Date the grant was announced.
    - CIA_Name: Chief Investigator A Name (e.g., CIA, Chief Investigator, Chief Investigator A (Project Lead)).
    - Grant_Title: Title of the grant or project.
    - Grant_Type: Type of grant (e.g., Investigator Grant, Ideas Grant, Funding Scheme).
    - Total_Amount: Total funding amount (e.g., Total amount awarded).
    - Broad_Research_Area: Broad area of research.
    - Field_of_Research: Specific field of research code or description (e.g., Fields of Research, All_Fields).
    - Plain_Description: A lay summary or media summary of the research.
    - Admin_Institution: Administering Institution.
    - Grant_Start_Year: The year the grant starts (e.g., Grant Start Date).
    - Grant_End_Date: The date the grant ends.
    - CIA_ORCID_ID: ORCID ID of the CIA.
    - Funding_Body: The funding body (usually NHMRC).
    - Grant_Status: Status of the grant (if available).
    - Investigators: List of all investigators (e.g., All_Investigators, Chief Investigator Team).
    - Participating_Institutions: List of participating institutions (e.g., All_Institutions, Participating Institutions).

    Current Headers from file: {current_headers}
    Target Headers: {TARGET_COLUMNS}
    
    Task:
    For each Target Header, identify the best matching Current Header.
    If no good match exists for a target header, do not include it in the mapping.
    Return a JSON object where the keys are the Current Headers and the values are the corresponding Target Headers.
    Only include the pairs that match.
    Example Output Format:
    {{
        "App ID": "Application_ID",
        "Title": "Grant_Title"
    }}
    """
    
    try:
        # Use the new google.genai API
        response = client.models.generate_content(
            model=model_name,
            contents=prompt
        )
        response_text = response.text
        
        # Simple parsing logic - in a real app, use a safer JSON parser or structured output mode if available
        import json
        
        # strip code blocks if present
        clean_text = response_text.strip()
        if clean_text.startswith("```json"):
            clean_text = clean_text[7:]
        if clean_text.startswith("```"):
            clean_text = clean_text[3:]
        if clean_text.endswith("```"):
            clean_text = clean_text[:-3]
            
        mapping = json.loads(clean_text)
        logger.info(f"Generated mapping: {mapping}")
        return mapping

    except Exception as e:
        logger.error(f"Error generating mapping with agent: {e}")
        # Fallback: exact string match (case insensitive)
        fallback_mapping = {}
        for ch in current_headers:
            # Skip if ch is not a string (e.g., int, float)
            if not isinstance(ch, str):
                continue
            for th in TARGET_COLUMNS:
                if ch.lower().replace("_", "").replace(" ", "") == th.lower().replace("_", ""):
                    fallback_mapping[ch] = th
                # Explicit override for Total
                elif ch.lower() == 'total' and th == 'Total_Amount':
                    fallback_mapping[ch] = th
        return fallback_mapping

def normalize_dataframe(df: pd.DataFrame, mapping: dict[str, str], source_filename: str) -> pd.DataFrame:
    """
    Renames columns based on mapping, adds missing columns, and ensures correct order.
    """
    # Rename columns
    df_renamed = df.rename(columns=mapping)
    
    # Check for and handle duplicate columns (can happen from the join operations)
    if df_renamed.columns.duplicated().any():
        logger.warning(f"Found duplicate columns in {source_filename}, keeping first occurrence")
        # Keep only the first occurrence of each column
        df_renamed = df_renamed.loc[:, ~df_renamed.columns.duplicated(keep='first')]
    
    # Add 'Source_File'
    df_renamed['Source_File'] = source_filename
    
    # Add 'Funding_Body' default if missing
    if 'Funding_Body' not in df_renamed.columns:
        # Heuristic: if filename contains 'ARC', it's ARC, else NHMRC
        if 'arc' in source_filename.lower():
            df_renamed['Funding_Body'] = 'ARC'
        else:
            df_renamed['Funding_Body'] = 'NHMRC'
            
    # Normalize Grant_Start_Year if it exists
    if 'Grant_Start_Year' in df_renamed.columns:
        try:
            # Convert to datetime, coerce errors to NaT
            dates = pd.to_datetime(df_renamed['Grant_Start_Year'], errors='coerce')
            # Extract year as string, fill NaT with original or empty
            # If conversion worked for most, use the year
            df_renamed['Grant_Start_Year'] = dates.dt.year.fillna(df_renamed['Grant_Start_Year']).astype(str).str.replace(r'\.0$', '', regex=True)
        except Exception as e:
            logger.warning(f"Could not normalize Grant_Start_Year: {e}")
            
    # Ensure all target columns exist
    for col in TARGET_COLUMNS:
        if col not in df_renamed.columns:
            df_renamed[col] = '' # Fill missing with empty string

    # Fallback: Populate Grant_Start_Year from Date_Announced if missing
    # We do this after ensuring columns exist so Grant_Start_Year is definitely there
    mask_missing_year = (df_renamed['Grant_Start_Year'] == '') | (df_renamed['Grant_Start_Year'].isna())
    if mask_missing_year.any() and 'Date_Announced' in df_renamed.columns:
        try:
             # Convert Date_Announced to datetime
             dates = pd.to_datetime(df_renamed.loc[mask_missing_year, 'Date_Announced'], errors='coerce')
             # Extract year
             years = dates.dt.year.astype('Int64').astype(str).replace('<NA>', 'nan').replace('nan', '')
             # Fill
             df_renamed.loc[mask_missing_year, 'Grant_Start_Year'] = years
             logger.info(f"Populated {len(years[years != ''])} missing Grant_Start_Year values from Date_Announced")
        except Exception as e:
            logger.warning(f"Could not populate Grant_Start_Year from Date_Announced: {e}")
            
    # Select and reorder
    df_final = df_renamed[TARGET_COLUMNS]
    
    return df_final
